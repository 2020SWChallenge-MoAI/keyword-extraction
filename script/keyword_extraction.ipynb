{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitkeywordextractionconda9a564a1e11b944df9d9937b883966b57",
   "display_name": "Python 3.7.7 64-bit ('keyword-extraction': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from konlpy.tag import Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.dirname(os.getcwd())\n",
    "\n",
    "input_dir = os.path.join(project_root, 'data', 'processed')\n",
    "output_dir = os.path.join(project_root, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        pos = ['{}/{}'.format(word,tag) for word, tag in pos if tag.startswith('NN')]\n",
    "        return pos\n",
    "\n",
    "tagger = Komoran()\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.6, tokenizer=Tokenizer(tagger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for filename in os.listdir(input_dir):\n",
    "    with open(os.path.join(input_dir, filename)) as f:\n",
    "        assert f.readline() == '@title\\n'\n",
    "        title = f.readline().strip()\n",
    "\n",
    "        assert f.readline() == '@content\\n'\n",
    "        contents = [line.strip() for line in f.readlines()]\n",
    "        tags = [tagger.pos(sent) for sent in contents]\n",
    "\n",
    "        documents.append({'title': title, 'contents': contents, 'tags': tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build DTM\n",
    "dt_matrices = vectorizer.fit_transform([' '.join(doc['contents']) for doc in documents])\n",
    "\n",
    "# build vocabulary\n",
    "idx2vocab = [vocab for vocab, idx in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])]\n",
    "\n",
    "# add additional informations to documents\n",
    "for doc, dtm in zip(documents, dt_matrices):\n",
    "    # add DTM\n",
    "    doc['dtm'] = dtm\n",
    "\n",
    "    # add sorted keywords\n",
    "    keywords = []\n",
    "    for idx, w in enumerate(dtm.toarray().squeeze()):\n",
    "        if w > 0.0:\n",
    "            keywords.append((idx2vocab[idx], w))\n",
    "        keywords = sorted(keywords, key=lambda word: word[1])\n",
    "        keywords.reverse()\n",
    "    doc['keywords'] = [w for w in keywords if w[1] > 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save documents and vocabulary\n",
    "with open(os.path.join(output_dir, 'documents.pkl'), 'wb') as f:\n",
    "    pickle.dump(documents, f)\n",
    "    \n",
    "with open(os.path.join(output_dir, 'vocab.pkl'), 'wb') as f:\n",
    "    pickle.dump(idx2vocab, f)"
   ]
  }
 ]
}