{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595043258084",
   "display_name": "Python 3.8.2 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(project_root, 'data', 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab(os.path.join(project_root, '.venv/lib/mecab/dic/mecab-ko-dic'))\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "originals = []\n",
    "for filename in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, filename)) as f:\n",
    "        assert f.readline() == '@title\\n'\n",
    "\n",
    "        title = f.readline().strip()\n",
    "\n",
    "        assert f.readline() == '@content\\n'\n",
    "        content = f.read()\n",
    "\n",
    "        nouns = tokenizer.nouns(content)\n",
    "        documents.append(' '.join(nouns))\n",
    "        originals.append({'title': title, 'nouns': nouns})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(documents)\n",
    "dtm = vectorizer.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx2vocab = [vocab for vocab, idx in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])]\n",
    "words = []\n",
    "for idx, w in enumerate(dtm[1].toarray().squeeze()):\n",
    "    if w > 0.0:\n",
    "        words.append((idx2vocab[idx], w))\n",
    "\n",
    "words = sorted(words, key=lambda word: word[1])\n",
    "words.reverse()\n",
    "\n",
    "print(words)\n",
    "print(originals[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}